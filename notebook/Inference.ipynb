{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPF1gktqXCbzE/bjD56x6i0"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "9HK48HUvB6zL"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9lZo0IYB8-h",
        "outputId": "b27b66f3-9eca-44fc-b437-cc403be94443"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/NLP-DL"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7JzujO_CFsP",
        "outputId": "527910fd-df38-489f-ff0b-40f5100a0adb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/NLP-DL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OZyY_0hChpn",
        "outputId": "4ab7499a-89d0-43d8-d184-23c73d69f702"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM  RNN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN"
      ],
      "metadata": {
        "id": "lfqI_JVl8nVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "fRlfprFr-YFQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "7OU3WhcF8t93"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_mappings = pickle.load(open(\"RNN/char_mappings.pkl\", \"rb\"))"
      ],
      "metadata": {
        "id": "YiO0qmLJ87r8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx_to_char = char_mappings['idx_to_char']\n",
        "char_to_idx = char_mappings['char_to_idx']"
      ],
      "metadata": {
        "id": "LXMttn4R9__L"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        embedded = self.embedding(x)\n",
        "        output, hidden = self.rnn(embedded, hidden)\n",
        "        output = self.fc(output)\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        device = next(self.parameters()).device  # Get the device of the model\n",
        "        return torch.zeros(1, batch_size, self.hidden_dim, device=device)"
      ],
      "metadata": {
        "id": "KKdwMjqA-GOR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_embedding_dim = 128\n",
        "rnn_hidden_dim = 256\n",
        "rnn_model = RNNModel(43, rnn_embedding_dim, rnn_hidden_dim)"
      ],
      "metadata": {
        "id": "RtRKJQkw-Kze"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_model = rnn_model.to(device)"
      ],
      "metadata": {
        "id": "3UsO19Nc-Tsi"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_model.load_state_dict(torch.load('RNN/rnn.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jl080tFK-Oq5",
        "outputId": "81a13bdb-7651-4d93-9ed9-667546489d9a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_lyrics(model, start_letter, max_length=100, temperature=1.0):\n",
        "    model.eval()\n",
        "\n",
        "    # Start with the given letter\n",
        "    input_seq = torch.tensor([char_to_idx[start_letter]], dtype=torch.long).unsqueeze(0).to(device)\n",
        "    hidden = model.init_hidden(1)\n",
        "    generated = [start_letter]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            output, hidden = model(input_seq, hidden)\n",
        "\n",
        "            # Apply temperature scaling before softmax\n",
        "            probs = torch.softmax(output[-1] / temperature, dim=-1)\n",
        "\n",
        "            # Sample the next character\n",
        "            next_char_idx = torch.multinomial(probs, 1).item()\n",
        "            next_char = idx_to_char[next_char_idx]\n",
        "            generated.append(next_char)\n",
        "\n",
        "            # Prepare next input\n",
        "            input_seq = torch.tensor([[next_char_idx]], dtype=torch.long).to(device)\n",
        "\n",
        "            if next_char == '\\n':  # Stop at newline (optional)\n",
        "                break\n",
        "\n",
        "    return ''.join(generated)\n"
      ],
      "metadata": {
        "id": "xcDZM44k-eth"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_lyrics(rnn_model, start_letter='a', max_length=50, temperature=0.8))\n",
        "print(generate_lyrics(rnn_model, start_letter='t', max_length=50, temperature=1.5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-2hGm-8-lXk",
        "outputId": "d32a2e52-4b43-4fbf-9f28-ae3cfc048bed"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "and you offi could ithere viemy don't got down all \n",
            "thigghing,but i berighberwixasbusry, cries woldwont\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM"
      ],
      "metadata": {
        "id": "GJBShZMk7Wo9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "VOOxOw367x0u"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = pickle.load(open(\"LSTM/lstm-word-vocab.pkl\", \"rb\"))"
      ],
      "metadata": {
        "id": "hjAE-xj07zS1"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch"
      ],
      "metadata": {
        "id": "Um48iBI-7YJl"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LyricsLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n",
        "        super(LyricsLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        out, hidden = self.lstm(x, hidden)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out, hidden"
      ],
      "metadata": {
        "id": "zzZhVPyS7ntc"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 64\n",
        "hidden_dim = 128\n",
        "num_layers = 2\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# Initialize model\n",
        "model = LyricsLSTM(vocab_size, embedding_dim, hidden_dim, num_layers).to(device)"
      ],
      "metadata": {
        "id": "u0TOLKan7ohQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('LSTM/lstm.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tCFFwS47-f0",
        "outputId": "fff864ce-ca51-416b-cdad-a2347087dda5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_mappings = pickle.load(open(\"LSTM/lstm-idx-mapping.pkl\", \"rb\"))\n",
        "idx_to_word = word_mappings['idx_to_word']\n",
        "word_to_idx = word_mappings['word_to_idx']"
      ],
      "metadata": {
        "id": "mAK_8sFC8KmS"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Inference"
      ],
      "metadata": {
        "id": "sP9VXlxb8GfD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "\n",
        "def generate_lyrics_using_lstm(starting_letter, max_words=50, temperature=1.0):\n",
        "    filtered_vocab = [word for word in vocab if word.startswith(starting_letter)]\n",
        "    if not filtered_vocab:\n",
        "        return \"No words found starting with this letter.\"\n",
        "\n",
        "    words = [random.choice(filtered_vocab)]\n",
        "\n",
        "    hidden = (torch.zeros(num_layers, 1, hidden_dim).to(device),\n",
        "              torch.zeros(num_layers, 1, hidden_dim).to(device))\n",
        "\n",
        "    for _ in range(max_words - 1):\n",
        "        input_seq = torch.tensor([[word_to_idx[words[-1]]]], dtype=torch.long).to(device)\n",
        "        output, hidden = model(input_seq, hidden)\n",
        "\n",
        "        output = output / temperature\n",
        "        probabilities = torch.nn.functional.softmax(output, dim=1)\n",
        "\n",
        "        predicted_idx = torch.multinomial(probabilities, 1).item()\n",
        "\n",
        "        next_word = idx_to_word.get(predicted_idx, \"<UNK>\")\n",
        "        words.append(next_word)\n",
        "\n",
        "    return ' '.join(words)"
      ],
      "metadata": {
        "id": "y3GFl2b98Dn7"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_lyrics_using_lstm('a', max_words=10, temperature=0.8))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oofvxdT18ee9",
        "outputId": "e5858cdc-dd1f-41a2-b69d-9e3edbed819b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "anakin my my clots, she was our cry on his\n"
          ]
        }
      ]
    }
  ]
}